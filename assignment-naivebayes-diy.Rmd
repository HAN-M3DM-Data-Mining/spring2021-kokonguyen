---
title: "Assigment - Naive Bayes DIY"
author:
  - name author here - Author
  - name reviewer here - Reviewer
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
   html_notebook:
    toc: true
    toc_depth: 2
---

```{r}
library(tidyverse)
library(tm)
library(caret)
library(wordcloud)
library(e1071)
```
---

## Business Understanding

The chosen dataset for this testing is the Fake News. This dataset indicating sources that are reliable or unreliable.I will use Naive Bayes Classifer to identify which is fake news and which isn't. I will count the amount of time a word appears in a headline - taking that this headline is from a fake news.Then afterwards find out the probability that the headline is fake,  compare to a real one. 

## Data Understanding
```{r}
url <- "https://raw.githubusercontent.com/HAN-M3DM-Data-Mining/assignments/master/datasets/NB-fakenews.csv"
rawDF <- read_csv(url)
head(rawDF)
```


The dataset has 5 variables (columns) and 20800 observations (rows)

After reading the dataset, we find out that variable ID, author and text do not contain any relevant information for our prediction. Hence, we will remove column 1,3 and 4 which represent these data.

```{r}
cleanDF <- rawDF[c(-1,-3,-4)] %>% na.omit
head(cleanDF)
```


The variable label is numeric value with 0: reliable and 1: unreliable. As it indicates whether the tittle belongs to the category "Reliable" or "Unreliable", we will convert it into a factor variable.

```{r}
cleanDF$label <- factor(cleanDF$label, levels = c("0", "1"), labels = c("Reliable", "Unreliable")) %>% relevel("Reliable")
class(cleanDF$label)
```

For visually inspect, we create a wordcloud for two variables. 

```{r}
Unreliable <- cleanDF %>% filter(label == "Unreliable")
Reliable <- cleanDF %>% filter(label == "Reliable")

wordcloud(Unreliable$title, max.words = 20, scale = c(4, 0.8), colors= c("indianred1","indianred2","indianred3","indianred"))
wordcloud(Reliable$title, max.words = 20, scale = c(4, 0.8), colors= c("lightsteelblue1","lightsteelblue2","lightsteelblue3","lightsteelblue"))
```
There is no significant difference between the number of reliable vs unreliable articles released. Based on the wordclouds, it is likely that the name of former US president his rival in the 2016 election would take place in many unreliable titles. While in the reliable side, "new","york" and "times" also appeared to be mentioned multiple times.


## Data Preparation

We create a corpus aims to refer to a collection of text document
```{r}
rawCorpus <- Corpus(VectorSource(cleanDF$title))
inspect(rawCorpus[1:3])
```
The corpus contains 20242 elements which matches with the number of rows in our dataset.
We will also change everything to lowercase and remove numbers as these will contain litle information on whether an article is reliable or not

```{r}
cleanCorpus <- rawCorpus %>% tm_map(tolower) %>% tm_map(removeNumbers)
inspect(cleanCorpus[1:3])
```
We also remove stopwords, punctuation and whitespaces

```{r}
cleanCorpus <- cleanCorpus %>% tm_map(tolower) %>% tm_map(removeWords, stopwords()) %>% tm_map(removePunctuation)
inspect(cleanCorpus[1:3])
```
Remove additional whitespace

```{r}
cleanCorpus <- cleanCorpus %>% tm_map(stripWhitespace)
inspect(cleanCorpus[1:3])
```
Compare with the raw version

```{r}
tibble(Raw = rawCorpus$content[1:3], Clean = cleanCorpus$content[1:3])
```

After cleaning up the text, we transform the messages to a matrix 

```{r}
cleanDTM <- cleanCorpus %>% DocumentTermMatrix
inspect(cleanDTM)
```
Dataset will be splitted into train and test sets

```{r}
# Create split indices
set.seed(1234)
trainIndex <- createDataPartition(cleanDF$label, p = .75, 
                                  list = FALSE, 
                                  times = 1)
head(trainIndex)
```



```{r}
# Apply split indices to DF
trainDF <- cleanDF[trainIndex, ]
testDF <- cleanDF[-trainIndex, ]

```
```{r}
# Apply split indices to Corpus
trainCorpus <- cleanCorpus[trainIndex]
testCorpus <- cleanCorpus[-trainIndex]

```

```{r}
# Apply split indices to DTM
trainDTM <- cleanDTM[trainIndex, ]
testDTM <- cleanDTM[-trainIndex, ]
```

Eliminate words with low frequencies

```{r}
freqWords <- trainDTM %>% findFreqTerms(10)
trainDTM <-  DocumentTermMatrix(trainCorpus, list(dictionary = freqWords))
testDTM <-  DocumentTermMatrix(testCorpus, list(dictionary = freqWords))
```

We will now transform the numerical matrix of words count into a factor that indicates whether the words appear in the document or not

```{r}
convert_counts <- function(x) {
  x <- ifelse(x > 0, 1, 0) %>% factor(levels = c(0,1), labels = c("No", "Yes"))
}

nColsDTM <- dim(trainDTM)[2]
trainDTM <- apply(trainDTM, MARGIN = 2, convert_counts)
testDTM <- apply(testDTM, MARGIN = 2, convert_counts)

head(trainDTM[,1:10])
```


## Modeling

```{r}
nbayesModel <-  naiveBayes(trainDTM, trainDF$label, laplace = 1)
```

```{r}
predVec <- predict(nbayesModel, testDTM)
confusionMatrix(predVec, testDF$label, positive = "Unreliable", dnn = c("Prediction", "True"))
```

## Evaluation and Deployment
The model has an accuracy rate of 92,3% which is quite a good result. Also noticed that in some cases the model still wrongly show a headline is reliable whilst it isnt and vice versa.